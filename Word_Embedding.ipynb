{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is testing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The sky is blue and beautiful.',\n",
       "       'Love this blue and beautiful sky!',\n",
       "       'The quick brown fox jumps over the lazy dog.',\n",
       "       \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
       "       'I love green eggs, ham, sausages and bacon!',\n",
       "       'The brown fox is quick and the blue dog is lazy!',\n",
       "       'The sky is very blue and the sky is very beautiful today',\n",
       "       'The dog is lazy but the brown fox is quick!'], dtype='<U66')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love green eggs, ham, sausages and bacon!'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = re.sub(r'[^a-zA-Z\\s]', '', corpus[4], re.I|re.A)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = doc.strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'green', 'eggs', 'ham', 'sausages', 'and', 'bacon']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = wpt.tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'green', 'eggs', 'ham', 'sausages', 'bacon']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love green eggs ham sausages bacon'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = ' '.join(filtered_tokens)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in 'C:\\\\Users\\\\vhuang\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\gutenberg'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = gutenberg.sents('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'King', 'James', 'Bible', ']'],\n",
       " ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'],\n",
       " ['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'king', 'james', 'bible'],\n",
       " ['the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible'],\n",
       " ['the', 'first', 'book', 'of', 'moses', 'called', 'genesis']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the king james bible',\n",
       " 'the old testament of the king james bible',\n",
       " 'the first book of moses called genesis',\n",
       " 'in the beginning god created the heaven and the earth',\n",
       " 'and the earth was without form and void and darkness was upon the face of the deep']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = filter(None, normalize_corpus(norm_bible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word2vec using CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shall': 1,\n",
       " 'unto': 2,\n",
       " 'lord': 3,\n",
       " 'thou': 4,\n",
       " 'thy': 5,\n",
       " 'god': 6,\n",
       " 'ye': 7,\n",
       " 'said': 8,\n",
       " 'thee': 9,\n",
       " 'upon': 10,\n",
       " 'man': 11,\n",
       " 'israel': 12,\n",
       " 'king': 13,\n",
       " 'son': 14,\n",
       " 'hath': 15,\n",
       " 'people': 16,\n",
       " 'came': 17,\n",
       " 'house': 18,\n",
       " 'come': 19,\n",
       " 'one': 20,\n",
       " 'children': 21,\n",
       " 'also': 22,\n",
       " 'day': 23,\n",
       " 'land': 24,\n",
       " 'men': 25,\n",
       " 'shalt': 26,\n",
       " 'let': 27,\n",
       " 'go': 28,\n",
       " 'hand': 29,\n",
       " 'saying': 30,\n",
       " 'us': 31,\n",
       " 'made': 32,\n",
       " 'even': 33,\n",
       " 'went': 34,\n",
       " 'behold': 35,\n",
       " 'saith': 36,\n",
       " 'every': 37,\n",
       " 'therefore': 38,\n",
       " 'things': 39,\n",
       " 'father': 40,\n",
       " 'sons': 41,\n",
       " 'hast': 42,\n",
       " 'david': 43,\n",
       " 'make': 44,\n",
       " 'say': 45,\n",
       " 'may': 46,\n",
       " 'earth': 47,\n",
       " 'jesus': 48,\n",
       " 'great': 49,\n",
       " 'name': 50,\n",
       " 'thine': 51,\n",
       " 'away': 52,\n",
       " 'put': 53,\n",
       " 'among': 54,\n",
       " 'thereof': 55,\n",
       " 'forth': 56,\n",
       " 'give': 57,\n",
       " 'neither': 58,\n",
       " 'take': 59,\n",
       " 'city': 60,\n",
       " 'days': 61,\n",
       " 'brought': 62,\n",
       " 'moses': 63,\n",
       " 'two': 64,\n",
       " 'heart': 65,\n",
       " 'pass': 66,\n",
       " 'judah': 67,\n",
       " 'jerusalem': 68,\n",
       " 'according': 69,\n",
       " 'know': 70,\n",
       " 'took': 71,\n",
       " 'thus': 72,\n",
       " 'offering': 73,\n",
       " 'bring': 74,\n",
       " 'good': 75,\n",
       " 'place': 76,\n",
       " 'word': 77,\n",
       " 'set': 78,\n",
       " 'sent': 79,\n",
       " 'yet': 80,\n",
       " 'like': 81,\n",
       " 'way': 82,\n",
       " 'eat': 83,\n",
       " 'mine': 84,\n",
       " 'heard': 85,\n",
       " 'called': 86,\n",
       " 'time': 87,\n",
       " 'evil': 88,\n",
       " 'holy': 89,\n",
       " 'egypt': 90,\n",
       " 'see': 91,\n",
       " 'hundred': 92,\n",
       " 'spake': 93,\n",
       " 'heaven': 94,\n",
       " 'christ': 95,\n",
       " 'done': 96,\n",
       " 'brethren': 97,\n",
       " 'many': 98,\n",
       " 'hear': 99,\n",
       " 'fire': 100,\n",
       " 'saw': 101,\n",
       " 'fathers': 102,\n",
       " 'words': 103,\n",
       " 'priest': 104,\n",
       " 'thing': 105,\n",
       " 'years': 106,\n",
       " 'law': 107,\n",
       " 'thousand': 108,\n",
       " 'speak': 109,\n",
       " 'voice': 110,\n",
       " 'spirit': 111,\n",
       " 'eyes': 112,\n",
       " 'cast': 113,\n",
       " 'given': 114,\n",
       " 'servant': 115,\n",
       " 'art': 116,\n",
       " 'together': 117,\n",
       " 'three': 118,\n",
       " 'servants': 119,\n",
       " 'answered': 120,\n",
       " 'ever': 121,\n",
       " 'might': 122,\n",
       " 'gave': 123,\n",
       " 'hands': 124,\n",
       " 'soul': 125,\n",
       " 'seven': 126,\n",
       " 'would': 127,\n",
       " 'another': 128,\n",
       " 'life': 129,\n",
       " 'cities': 130,\n",
       " 'blood': 131,\n",
       " 'first': 132,\n",
       " 'sin': 133,\n",
       " 'commanded': 134,\n",
       " 'side': 135,\n",
       " 'without': 136,\n",
       " 'sword': 137,\n",
       " 'peace': 138,\n",
       " 'mouth': 139,\n",
       " 'saul': 140,\n",
       " 'flesh': 141,\n",
       " 'work': 142,\n",
       " 'gold': 143,\n",
       " 'face': 144,\n",
       " 'high': 145,\n",
       " 'wife': 146,\n",
       " 'found': 147,\n",
       " 'brother': 148,\n",
       " 'sea': 149,\n",
       " 'priests': 150,\n",
       " 'fear': 151,\n",
       " 'glory': 152,\n",
       " 'water': 153,\n",
       " 'old': 154,\n",
       " 'altar': 155,\n",
       " 'jacob': 156,\n",
       " 'death': 157,\n",
       " 'year': 158,\n",
       " 'drink': 159,\n",
       " 'burnt': 160,\n",
       " 'midst': 161,\n",
       " 'woman': 162,\n",
       " 'head': 163,\n",
       " 'congregation': 164,\n",
       " 'keep': 165,\n",
       " 'dead': 166,\n",
       " 'bread': 167,\n",
       " 'right': 168,\n",
       " 'none': 169,\n",
       " 'aaron': 170,\n",
       " 'left': 171,\n",
       " 'toward': 172,\n",
       " 'five': 173,\n",
       " 'wherefore': 174,\n",
       " 'wicked': 175,\n",
       " 'kingdom': 176,\n",
       " 'kings': 177,\n",
       " 'yea': 178,\n",
       " 'dwell': 179,\n",
       " 'stood': 180,\n",
       " 'taken': 181,\n",
       " 'nations': 182,\n",
       " 'sight': 183,\n",
       " 'four': 184,\n",
       " 'tabernacle': 185,\n",
       " 'cause': 186,\n",
       " 'daughter': 187,\n",
       " 'silver': 188,\n",
       " 'round': 189,\n",
       " 'die': 190,\n",
       " 'mother': 191,\n",
       " 'cut': 192,\n",
       " 'whose': 193,\n",
       " 'pray': 194,\n",
       " 'love': 195,\n",
       " 'righteousness': 196,\n",
       " 'night': 197,\n",
       " 'end': 198,\n",
       " 'wilderness': 199,\n",
       " 'solomon': 200,\n",
       " 'blessed': 201,\n",
       " 'young': 202,\n",
       " 'hosts': 203,\n",
       " 'deliver': 204,\n",
       " 'twenty': 205,\n",
       " 'judgment': 206,\n",
       " 'babylon': 207,\n",
       " 'covenant': 208,\n",
       " 'field': 209,\n",
       " 'meat': 210,\n",
       " 'delivered': 211,\n",
       " 'waters': 212,\n",
       " 'turned': 213,\n",
       " 'much': 214,\n",
       " 'world': 215,\n",
       " 'surely': 216,\n",
       " 'spoken': 217,\n",
       " 'mighty': 218,\n",
       " 'turn': 219,\n",
       " 'chief': 220,\n",
       " 'cometh': 221,\n",
       " 'told': 222,\n",
       " 'seed': 223,\n",
       " 'laid': 224,\n",
       " 'seen': 225,\n",
       " 'written': 226,\n",
       " 'mercy': 227,\n",
       " 'gate': 228,\n",
       " 'rest': 229,\n",
       " 'iniquity': 230,\n",
       " 'princes': 231,\n",
       " 'light': 232,\n",
       " 'pharaoh': 233,\n",
       " 'stand': 234,\n",
       " 'power': 235,\n",
       " 'enemies': 236,\n",
       " 'gathered': 237,\n",
       " 'levites': 238,\n",
       " 'offerings': 239,\n",
       " 'return': 240,\n",
       " 'mount': 241,\n",
       " 'well': 242,\n",
       " 'destroy': 243,\n",
       " 'full': 244,\n",
       " 'feet': 245,\n",
       " 'strong': 246,\n",
       " 'jews': 247,\n",
       " 'philistines': 248,\n",
       " 'fall': 249,\n",
       " 'daughters': 250,\n",
       " 'whole': 251,\n",
       " 'month': 252,\n",
       " 'joseph': 253,\n",
       " 'ten': 254,\n",
       " 'abraham': 255,\n",
       " 'live': 256,\n",
       " 'wilt': 257,\n",
       " 'faith': 258,\n",
       " 'wise': 259,\n",
       " 'gods': 260,\n",
       " 'fell': 261,\n",
       " 'multitude': 262,\n",
       " 'prophet': 263,\n",
       " 'praise': 264,\n",
       " 'disciples': 265,\n",
       " 'little': 266,\n",
       " 'seek': 267,\n",
       " 'tribe': 268,\n",
       " 'lest': 269,\n",
       " 'strength': 270,\n",
       " 'concerning': 271,\n",
       " 'lay': 272,\n",
       " 'inheritance': 273,\n",
       " 'prophets': 274,\n",
       " 'righteous': 275,\n",
       " 'truth': 276,\n",
       " 'offer': 277,\n",
       " 'works': 278,\n",
       " 'send': 279,\n",
       " 'anger': 280,\n",
       " 'though': 281,\n",
       " 'wine': 282,\n",
       " 'save': 283,\n",
       " 'wisdom': 284,\n",
       " 'ark': 285,\n",
       " 'therein': 286,\n",
       " 'morning': 287,\n",
       " 'shew': 288,\n",
       " 'smote': 289,\n",
       " 'dwelt': 290,\n",
       " 'begat': 291,\n",
       " 'war': 292,\n",
       " 'nothing': 293,\n",
       " 'known': 294,\n",
       " 'sacrifice': 295,\n",
       " 'bear': 296,\n",
       " 'tell': 297,\n",
       " 'places': 298,\n",
       " 'joshua': 299,\n",
       " 'thyself': 300,\n",
       " 'cubits': 301,\n",
       " 'gone': 302,\n",
       " 'part': 303,\n",
       " 'walk': 304,\n",
       " 'departed': 305,\n",
       " 'near': 306,\n",
       " 'long': 307,\n",
       " 'fruit': 308,\n",
       " 'serve': 309,\n",
       " 'book': 310,\n",
       " 'doth': 311,\n",
       " 'poor': 312,\n",
       " 'temple': 313,\n",
       " 'ways': 314,\n",
       " 'six': 315,\n",
       " 'child': 316,\n",
       " 'inhabitants': 317,\n",
       " 'tree': 318,\n",
       " 'angel': 319,\n",
       " 'oil': 320,\n",
       " 'died': 321,\n",
       " 'cried': 322,\n",
       " 'wrath': 323,\n",
       " 'jordan': 324,\n",
       " 'manner': 325,\n",
       " 'certain': 326,\n",
       " 'slew': 327,\n",
       " 'call': 328,\n",
       " 'unclean': 329,\n",
       " 'afraid': 330,\n",
       " 'rejoice': 331,\n",
       " 'host': 332,\n",
       " 'ground': 333,\n",
       " 'stone': 334,\n",
       " 'judge': 335,\n",
       " 'sat': 336,\n",
       " 'door': 337,\n",
       " 'sheep': 338,\n",
       " 'twelve': 339,\n",
       " 'within': 340,\n",
       " 'broken': 341,\n",
       " 'hold': 342,\n",
       " 'third': 343,\n",
       " 'bare': 344,\n",
       " 'slain': 345,\n",
       " 'cannot': 346,\n",
       " 'whosoever': 347,\n",
       " 'returned': 348,\n",
       " 'women': 349,\n",
       " 'beast': 350,\n",
       " 'cry': 351,\n",
       " 'master': 352,\n",
       " 'river': 353,\n",
       " 'wall': 354,\n",
       " 'elders': 355,\n",
       " 'began': 356,\n",
       " 'mountains': 357,\n",
       " 'country': 358,\n",
       " 'stones': 359,\n",
       " 'number': 360,\n",
       " 'ephraim': 361,\n",
       " 'commandment': 362,\n",
       " 'second': 363,\n",
       " 'reigned': 364,\n",
       " 'throne': 365,\n",
       " 'receive': 366,\n",
       " 'kept': 367,\n",
       " 'paul': 368,\n",
       " 'thirty': 369,\n",
       " 'body': 370,\n",
       " 'families': 371,\n",
       " 'far': 372,\n",
       " 'sins': 373,\n",
       " 'knowledge': 374,\n",
       " 'whether': 375,\n",
       " 'commandments': 376,\n",
       " 'battle': 377,\n",
       " 'moreover': 378,\n",
       " 'till': 379,\n",
       " 'knew': 380,\n",
       " 'built': 381,\n",
       " 'arose': 382,\n",
       " 'reign': 383,\n",
       " 'wherein': 384,\n",
       " 'destroyed': 385,\n",
       " 'moab': 386,\n",
       " 'could': 387,\n",
       " 'gather': 388,\n",
       " 'benjamin': 389,\n",
       " 'joy': 390,\n",
       " 'grace': 391,\n",
       " 'peter': 392,\n",
       " 'build': 393,\n",
       " 'salvation': 394,\n",
       " 'throughout': 395,\n",
       " 'darkness': 396,\n",
       " 'passed': 397,\n",
       " 'able': 398,\n",
       " 'sun': 399,\n",
       " 'received': 400,\n",
       " 'born': 401,\n",
       " 'forty': 402,\n",
       " 'lo': 403,\n",
       " 'border': 404,\n",
       " 'trees': 405,\n",
       " 'filled': 406,\n",
       " 'fifty': 407,\n",
       " 'understanding': 408,\n",
       " 'east': 409,\n",
       " 'lifted': 410,\n",
       " 'find': 411,\n",
       " 'beasts': 412,\n",
       " 'look': 413,\n",
       " 'lie': 414,\n",
       " 'vessels': 415,\n",
       " 'cattle': 416,\n",
       " 'back': 417,\n",
       " 'zion': 418,\n",
       " 'whatsoever': 419,\n",
       " 'hearken': 420,\n",
       " 'ears': 421,\n",
       " 'new': 422,\n",
       " 'heathen': 423,\n",
       " 'enter': 424,\n",
       " 'arise': 425,\n",
       " 'desolate': 426,\n",
       " 'manasseh': 427,\n",
       " 'jeremiah': 428,\n",
       " 'living': 429,\n",
       " 'remember': 430,\n",
       " 'thence': 431,\n",
       " 'sake': 432,\n",
       " 'nation': 433,\n",
       " 'times': 434,\n",
       " 'carried': 435,\n",
       " 'joab': 436,\n",
       " 'fled': 437,\n",
       " 'gates': 438,\n",
       " 'samuel': 439,\n",
       " 'looked': 440,\n",
       " 'offered': 441,\n",
       " 'south': 442,\n",
       " 'honour': 443,\n",
       " 'counsel': 444,\n",
       " 'believe': 445,\n",
       " 'wood': 446,\n",
       " 'rise': 447,\n",
       " 'break': 448,\n",
       " 'verily': 449,\n",
       " 'valley': 450,\n",
       " 'money': 451,\n",
       " 'captain': 452,\n",
       " 'john': 453,\n",
       " 'burn': 454,\n",
       " 'sanctuary': 455,\n",
       " 'opened': 456,\n",
       " 'mountain': 457,\n",
       " 'half': 458,\n",
       " 'houses': 459,\n",
       " 'camp': 460,\n",
       " 'sabbath': 461,\n",
       " 'goeth': 462,\n",
       " 'neighbour': 463,\n",
       " 'become': 464,\n",
       " 'shewed': 465,\n",
       " 'heavens': 466,\n",
       " 'wives': 467,\n",
       " 'trust': 468,\n",
       " 'clean': 469,\n",
       " 'must': 470,\n",
       " 'isaac': 471,\n",
       " 'statutes': 472,\n",
       " 'service': 473,\n",
       " 'meet': 474,\n",
       " 'rose': 475,\n",
       " 'stranger': 476,\n",
       " 'witness': 477,\n",
       " 'beside': 478,\n",
       " 'north': 479,\n",
       " 'fat': 480,\n",
       " 'answer': 481,\n",
       " 'hid': 482,\n",
       " 'incense': 483,\n",
       " 'gentiles': 484,\n",
       " 'tongue': 485,\n",
       " 'numbered': 486,\n",
       " 'hezekiah': 487,\n",
       " 'bless': 488,\n",
       " 'judgments': 489,\n",
       " 'husband': 490,\n",
       " 'wickedness': 491,\n",
       " 'sought': 492,\n",
       " 'maketh': 493,\n",
       " 'captivity': 494,\n",
       " 'help': 495,\n",
       " 'kill': 496,\n",
       " 'brass': 497,\n",
       " 'appointed': 498,\n",
       " 'depart': 499,\n",
       " 'giveth': 500,\n",
       " 'hope': 501,\n",
       " 'open': 502,\n",
       " 'smite': 503,\n",
       " 'whither': 504,\n",
       " 'wind': 505,\n",
       " 'feast': 506,\n",
       " 'family': 507,\n",
       " 'ashamed': 508,\n",
       " 'didst': 509,\n",
       " 'court': 510,\n",
       " 'pieces': 511,\n",
       " 'chosen': 512,\n",
       " 'samaria': 513,\n",
       " 'seventh': 514,\n",
       " 'perish': 515,\n",
       " 'ear': 516,\n",
       " 'jonathan': 517,\n",
       " 'walked': 518,\n",
       " 'lips': 519,\n",
       " 'captains': 520,\n",
       " 'sing': 521,\n",
       " 'rock': 522,\n",
       " 'assyria': 523,\n",
       " 'get': 524,\n",
       " 'asked': 525,\n",
       " 'sinned': 526,\n",
       " 'generations': 527,\n",
       " 'slay': 528,\n",
       " 'spoil': 529,\n",
       " 'presence': 530,\n",
       " 'seeing': 531,\n",
       " 'better': 532,\n",
       " 'eye': 533,\n",
       " 'leave': 534,\n",
       " 'sister': 535,\n",
       " 'believed': 536,\n",
       " 'suburbs': 537,\n",
       " 'mayest': 538,\n",
       " 'beloved': 539,\n",
       " 'lot': 540,\n",
       " 'hearts': 541,\n",
       " 'fine': 542,\n",
       " 'firstborn': 543,\n",
       " 'chariots': 544,\n",
       " 'tribes': 545,\n",
       " 'flock': 546,\n",
       " 'sit': 547,\n",
       " 'horses': 548,\n",
       " 'vain': 549,\n",
       " 'desire': 550,\n",
       " 'wherewith': 551,\n",
       " 'enemy': 552,\n",
       " 'heads': 553,\n",
       " 'spread': 554,\n",
       " 'ghost': 555,\n",
       " 'ask': 556,\n",
       " 'trouble': 557,\n",
       " 'prayer': 558,\n",
       " 'dust': 559,\n",
       " 'alone': 560,\n",
       " 'sweet': 561,\n",
       " 'worship': 562,\n",
       " 'teach': 563,\n",
       " 'generation': 564,\n",
       " 'entered': 565,\n",
       " 'cloud': 566,\n",
       " 'lamb': 567,\n",
       " 'fight': 568,\n",
       " 'bullock': 569,\n",
       " 'absalom': 570,\n",
       " 'gospel': 571,\n",
       " 'became': 572,\n",
       " 'buried': 573,\n",
       " 'possess': 574,\n",
       " 'present': 575,\n",
       " 'likewise': 576,\n",
       " 'eaten': 577,\n",
       " 'shut': 578,\n",
       " 'covered': 579,\n",
       " 'prince': 580,\n",
       " 'wait': 581,\n",
       " 'beginning': 582,\n",
       " 'lift': 583,\n",
       " 'flee': 584,\n",
       " 'possession': 585,\n",
       " 'command': 586,\n",
       " 'bound': 587,\n",
       " 'followed': 588,\n",
       " 'linen': 589,\n",
       " 'saved': 590,\n",
       " 'jeroboam': 591,\n",
       " 'garments': 592,\n",
       " 'lion': 593,\n",
       " 'rain': 594,\n",
       " 'charge': 595,\n",
       " 'corn': 596,\n",
       " 'knoweth': 597,\n",
       " 'image': 598,\n",
       " 'iron': 599,\n",
       " 'curse': 600,\n",
       " 'oxen': 601,\n",
       " 'morrow': 602,\n",
       " 'prepared': 603,\n",
       " 'clothes': 604,\n",
       " 'utterly': 605,\n",
       " 'idols': 606,\n",
       " 'bow': 607,\n",
       " 'still': 608,\n",
       " 'ready': 609,\n",
       " 'coming': 610,\n",
       " 'esau': 611,\n",
       " 'gilead': 612,\n",
       " 'wrought': 613,\n",
       " 'nigh': 614,\n",
       " 'minister': 615,\n",
       " 'bones': 616,\n",
       " 'perfect': 617,\n",
       " 'portion': 618,\n",
       " 'shame': 619,\n",
       " 'woe': 620,\n",
       " 'tent': 621,\n",
       " 'ram': 622,\n",
       " 'sore': 623,\n",
       " 'loved': 624,\n",
       " 'riches': 625,\n",
       " 'burned': 626,\n",
       " 'plague': 627,\n",
       " 'anointed': 628,\n",
       " 'everlasting': 629,\n",
       " 'egyptians': 630,\n",
       " 'ought': 631,\n",
       " 'goats': 632,\n",
       " 'pure': 633,\n",
       " 'names': 634,\n",
       " 'liveth': 635,\n",
       " 'famine': 636,\n",
       " 'consumed': 637,\n",
       " 'shekels': 638,\n",
       " 'doeth': 639,\n",
       " 'suffer': 640,\n",
       " 'nevertheless': 641,\n",
       " 'foot': 642,\n",
       " 'small': 643,\n",
       " 'thither': 644,\n",
       " 'declare': 645,\n",
       " 'saints': 646,\n",
       " 'caused': 647,\n",
       " 'angels': 648,\n",
       " 'stead': 649,\n",
       " 'mind': 650,\n",
       " 'threescore': 651,\n",
       " 'destruction': 652,\n",
       " 'ahab': 653,\n",
       " 'hour': 654,\n",
       " 'going': 655,\n",
       " 'committed': 656,\n",
       " 'remnant': 657,\n",
       " 'removed': 658,\n",
       " 'canaan': 659,\n",
       " 'top': 660,\n",
       " 'ammon': 661,\n",
       " 'bed': 662,\n",
       " 'write': 663,\n",
       " 'understand': 664,\n",
       " 'ass': 665,\n",
       " 'carry': 666,\n",
       " 'prison': 667,\n",
       " 'prophesy': 668,\n",
       " 'breadth': 669,\n",
       " 'wash': 670,\n",
       " 'knowest': 671,\n",
       " 'glad': 672,\n",
       " 'pillars': 673,\n",
       " 'sound': 674,\n",
       " 'reproach': 675,\n",
       " 'labour': 676,\n",
       " 'pit': 677,\n",
       " 'increase': 678,\n",
       " 'noise': 679,\n",
       " 'greatly': 680,\n",
       " 'alive': 681,\n",
       " 'hate': 682,\n",
       " 'sick': 683,\n",
       " 'fourth': 684,\n",
       " 'garment': 685,\n",
       " 'early': 686,\n",
       " 'company': 687,\n",
       " 'never': 688,\n",
       " 'ruler': 689,\n",
       " 'pharisees': 690,\n",
       " 'follow': 691,\n",
       " 'last': 692,\n",
       " 'rod': 693,\n",
       " 'raised': 694,\n",
       " 'drew': 695,\n",
       " 'poured': 696,\n",
       " 'fast': 697,\n",
       " 'edom': 698,\n",
       " 'jehoshaphat': 699,\n",
       " 'vanity': 700,\n",
       " 'daniel': 701,\n",
       " 'sleep': 702,\n",
       " 'pitched': 703,\n",
       " 'hide': 704,\n",
       " 'fulfilled': 705,\n",
       " 'army': 706,\n",
       " 'trespass': 707,\n",
       " 'faithful': 708,\n",
       " 'hearkened': 709,\n",
       " 'continually': 710,\n",
       " 'tenth': 711,\n",
       " 'rich': 712,\n",
       " 'abide': 713,\n",
       " 'lambs': 714,\n",
       " 'feed': 715,\n",
       " 'sold': 716,\n",
       " 'blind': 717,\n",
       " 'prepare': 718,\n",
       " 'atonement': 719,\n",
       " 'taught': 720,\n",
       " 'eight': 721,\n",
       " 'abroad': 722,\n",
       " 'flocks': 723,\n",
       " 'thought': 724,\n",
       " 'matter': 725,\n",
       " 'heed': 726,\n",
       " 'true': 727,\n",
       " 'fallen': 728,\n",
       " 'reward': 729,\n",
       " 'healed': 730,\n",
       " 'messengers': 731,\n",
       " 'remain': 732,\n",
       " 'sacrifices': 733,\n",
       " 'bringeth': 734,\n",
       " 'souls': 735,\n",
       " 'vision': 736,\n",
       " 'bowed': 737,\n",
       " 'strangers': 738,\n",
       " 'rulers': 739,\n",
       " 'greater': 740,\n",
       " 'length': 741,\n",
       " 'rams': 742,\n",
       " 'ones': 743,\n",
       " 'rivers': 744,\n",
       " 'hill': 745,\n",
       " 'skin': 746,\n",
       " 'church': 747,\n",
       " 'sware': 748,\n",
       " 'draw': 749,\n",
       " 'precious': 750,\n",
       " 'strange': 751,\n",
       " 'abomination': 752,\n",
       " 'sign': 753,\n",
       " 'passover': 754,\n",
       " 'testimony': 755,\n",
       " 'wings': 756,\n",
       " 'abominations': 757,\n",
       " 'forsaken': 758,\n",
       " 'moved': 759,\n",
       " 'plain': 760,\n",
       " 'affliction': 761,\n",
       " 'white': 762,\n",
       " 'branches': 763,\n",
       " 'villages': 764,\n",
       " 'commit': 765,\n",
       " 'syria': 766,\n",
       " 'simon': 767,\n",
       " 'established': 768,\n",
       " 'served': 769,\n",
       " 'behind': 770,\n",
       " 'feared': 771,\n",
       " 'dream': 772,\n",
       " 'reuben': 773,\n",
       " 'except': 774,\n",
       " 'speaketh': 775,\n",
       " 'eleazar': 776,\n",
       " 'taketh': 777,\n",
       " 'shouldest': 778,\n",
       " 'clothed': 779,\n",
       " 'faces': 780,\n",
       " 'hebron': 781,\n",
       " 'amorites': 782,\n",
       " 'shadow': 783,\n",
       " 'prey': 784,\n",
       " 'brake': 785,\n",
       " 'table': 786,\n",
       " 'thanks': 787,\n",
       " 'whereof': 788,\n",
       " 'cursed': 789,\n",
       " 'dan': 790,\n",
       " 'levi': 791,\n",
       " 'gad': 792,\n",
       " 'aside': 793,\n",
       " 'cover': 794,\n",
       " 'galilee': 795,\n",
       " 'dry': 796,\n",
       " 'whence': 797,\n",
       " 'scattered': 798,\n",
       " 'stretched': 799,\n",
       " 'images': 800,\n",
       " 'defiled': 801,\n",
       " 'reason': 802,\n",
       " 'run': 803,\n",
       " 'devour': 804,\n",
       " 'lebanon': 805,\n",
       " 'doors': 806,\n",
       " 'ship': 807,\n",
       " 'youth': 808,\n",
       " 'cease': 809,\n",
       " 'appeared': 810,\n",
       " 'indeed': 811,\n",
       " 'favour': 812,\n",
       " 'since': 813,\n",
       " 'sanctify': 814,\n",
       " 'read': 815,\n",
       " 'fool': 816,\n",
       " 'sorrow': 817,\n",
       " 'vineyard': 818,\n",
       " 'wept': 819,\n",
       " 'womb': 820,\n",
       " 'obey': 821,\n",
       " 'fury': 822,\n",
       " 'abode': 823,\n",
       " 'measure': 824,\n",
       " 'others': 825,\n",
       " 'divided': 826,\n",
       " 'west': 827,\n",
       " 'worshipped': 828,\n",
       " 'worthy': 829,\n",
       " 'grave': 830,\n",
       " 'upright': 831,\n",
       " 'cup': 832,\n",
       " 'secret': 833,\n",
       " 'abundance': 834,\n",
       " 'scribes': 835,\n",
       " 'elijah': 836,\n",
       " 'blessing': 837,\n",
       " 'horns': 838,\n",
       " 'led': 839,\n",
       " 'troubled': 840,\n",
       " 'killed': 841,\n",
       " 'beseech': 842,\n",
       " 'arm': 843,\n",
       " 'consider': 844,\n",
       " 'rule': 845,\n",
       " 'wast': 846,\n",
       " 'tents': 847,\n",
       " 'afterward': 848,\n",
       " 'hither': 849,\n",
       " 'abimelech': 850,\n",
       " 'golden': 851,\n",
       " 'kindled': 852,\n",
       " 'thereon': 853,\n",
       " 'rent': 854,\n",
       " 'crown': 855,\n",
       " 'walls': 856,\n",
       " 'acts': 857,\n",
       " 'chambers': 858,\n",
       " 'chaldeans': 859,\n",
       " 'deep': 860,\n",
       " 'comfort': 861,\n",
       " 'hills': 862,\n",
       " 'bethel': 863,\n",
       " 'soon': 864,\n",
       " 'prayed': 865,\n",
       " 'loveth': 866,\n",
       " 'streets': 867,\n",
       " 'cherubims': 868,\n",
       " 'asses': 869,\n",
       " 'shechem': 870,\n",
       " 'chariot': 871,\n",
       " 'fail': 872,\n",
       " 'parts': 873,\n",
       " 'fought': 874,\n",
       " 'false': 875,\n",
       " 'ox': 876,\n",
       " 'hair': 877,\n",
       " 'waste': 878,\n",
       " 'baal': 879,\n",
       " 'exalted': 880,\n",
       " 'howbeit': 881,\n",
       " 'judged': 882,\n",
       " 'loins': 883,\n",
       " 'pour': 884,\n",
       " 'daily': 885,\n",
       " 'smitten': 886,\n",
       " 'burden': 887,\n",
       " 'jericho': 888,\n",
       " 'balaam': 889,\n",
       " 'abner': 890,\n",
       " 'grass': 891,\n",
       " 'fifth': 892,\n",
       " 'dominion': 893,\n",
       " 'sanctified': 894,\n",
       " 'hurt': 895,\n",
       " 'always': 896,\n",
       " 'height': 897,\n",
       " 'inherit': 898,\n",
       " 'thousands': 899,\n",
       " 'neck': 900,\n",
       " 'pleased': 901,\n",
       " 'vine': 902,\n",
       " 'think': 903,\n",
       " 'redeemed': 904,\n",
       " 'coast': 905,\n",
       " 'blemish': 906,\n",
       " 'wrote': 907,\n",
       " 'treasures': 908,\n",
       " 'rather': 909,\n",
       " 'zedekiah': 910,\n",
       " 'seventy': 911,\n",
       " 'upward': 912,\n",
       " 'harvest': 913,\n",
       " 'abram': 914,\n",
       " 'ran': 915,\n",
       " 'pleasure': 916,\n",
       " 'household': 917,\n",
       " 'unleavened': 918,\n",
       " 'order': 919,\n",
       " 'oath': 920,\n",
       " 'speaking': 921,\n",
       " 'watch': 922,\n",
       " 'syrians': 923,\n",
       " 'preached': 924,\n",
       " 'devil': 925,\n",
       " 'evening': 926,\n",
       " 'burning': 927,\n",
       " 'despised': 928,\n",
       " 'next': 929,\n",
       " 'deal': 930,\n",
       " 'swear': 931,\n",
       " 'journey': 932,\n",
       " 'hated': 933,\n",
       " 'loud': 934,\n",
       " 'governor': 935,\n",
       " 'job': 936,\n",
       " 'trumpet': 937,\n",
       " 'asa': 938,\n",
       " 'nebuchadnezzar': 939,\n",
       " 'mordecai': 940,\n",
       " 'baptized': 941,\n",
       " 'lieth': 942,\n",
       " 'captive': 943,\n",
       " 'escape': 944,\n",
       " 'yoke': 945,\n",
       " 'gift': 946,\n",
       " 'raise': 947,\n",
       " 'months': 948,\n",
       " 'horsemen': 949,\n",
       " 'fields': 950,\n",
       " 'choose': 951,\n",
       " 'bashan': 952,\n",
       " 'jehu': 953,\n",
       " 'endureth': 954,\n",
       " 'apostles': 955,\n",
       " 'lived': 956,\n",
       " 'escaped': 957,\n",
       " 'weight': 958,\n",
       " 'lead': 959,\n",
       " 'officers': 960,\n",
       " 'habitation': 961,\n",
       " 'free': 962,\n",
       " 'flour': 963,\n",
       " 'forsake': 964,\n",
       " 'elisha': 965,\n",
       " 'thoughts': 966,\n",
       " 'violence': 967,\n",
       " 'remembered': 968,\n",
       " 'nakedness': 969,\n",
       " 'damascus': 970,\n",
       " 'exceeding': 971,\n",
       " 'dealt': 972,\n",
       " 'named': 973,\n",
       " 'raiment': 974,\n",
       " 'lying': 975,\n",
       " 'drive': 976,\n",
       " 'seat': 977,\n",
       " 'dwelleth': 978,\n",
       " 'kingdoms': 979,\n",
       " 'pleasant': 980,\n",
       " 'slaughter': 981,\n",
       " 'persons': 982,\n",
       " 'edge': 983,\n",
       " 'person': 984,\n",
       " 'season': 985,\n",
       " 'consume': 986,\n",
       " 'buy': 987,\n",
       " 'honey': 988,\n",
       " 'eateth': 989,\n",
       " 'tables': 990,\n",
       " 'iniquities': 991,\n",
       " 'satan': 992,\n",
       " 'esther': 993,\n",
       " 'pilate': 994,\n",
       " 'food': 995,\n",
       " 'fowls': 996,\n",
       " 'canaanites': 997,\n",
       " 'countries': 998,\n",
       " 'almighty': 999,\n",
       " 'haste': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = tokenizer.word_index\n",
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAD'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 1154, 5766],\n",
       " [154, 2450, 13, 1154, 5766],\n",
       " [132, 310, 63, 86, 8480],\n",
       " [582, 6, 1180, 94, 47],\n",
       " [47, 136, 1883, 1884, 396, 10, 144, 860]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grace lord jesus christ'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grace', 'lord', 'jesus', 'christ']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text_to_word_sequence(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i]\n",
    "                                 for i in range(start, end)\n",
    "                                 if 0 <= i < sentence_length\n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size,\n",
    "                                        vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-e7856a992586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mgenerate_context_word_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed {} (context, word) pairs'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1254\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    for x, y  in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12425, 100)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 12425)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12425,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>0.542385</td>\n",
       "      <td>-0.297696</td>\n",
       "      <td>-0.974339</td>\n",
       "      <td>-0.637744</td>\n",
       "      <td>-0.677178</td>\n",
       "      <td>0.773788</td>\n",
       "      <td>-0.769360</td>\n",
       "      <td>-0.685529</td>\n",
       "      <td>0.758910</td>\n",
       "      <td>0.539016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.461902</td>\n",
       "      <td>-0.355086</td>\n",
       "      <td>0.454457</td>\n",
       "      <td>0.599683</td>\n",
       "      <td>0.033022</td>\n",
       "      <td>-0.347140</td>\n",
       "      <td>-0.250258</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>0.553953</td>\n",
       "      <td>0.660755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>0.569215</td>\n",
       "      <td>-0.628994</td>\n",
       "      <td>-0.764580</td>\n",
       "      <td>-0.506907</td>\n",
       "      <td>-0.826138</td>\n",
       "      <td>0.275083</td>\n",
       "      <td>-0.601900</td>\n",
       "      <td>-0.503314</td>\n",
       "      <td>0.685744</td>\n",
       "      <td>0.364418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689491</td>\n",
       "      <td>-0.305249</td>\n",
       "      <td>0.546823</td>\n",
       "      <td>0.690898</td>\n",
       "      <td>0.119913</td>\n",
       "      <td>-0.166880</td>\n",
       "      <td>-0.507530</td>\n",
       "      <td>0.522185</td>\n",
       "      <td>0.071481</td>\n",
       "      <td>0.323034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>0.353029</td>\n",
       "      <td>-0.589287</td>\n",
       "      <td>-0.441116</td>\n",
       "      <td>-0.433910</td>\n",
       "      <td>-0.473335</td>\n",
       "      <td>0.357725</td>\n",
       "      <td>-0.486547</td>\n",
       "      <td>-0.431751</td>\n",
       "      <td>0.626495</td>\n",
       "      <td>0.349939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384809</td>\n",
       "      <td>-0.363280</td>\n",
       "      <td>0.142838</td>\n",
       "      <td>0.070567</td>\n",
       "      <td>0.219064</td>\n",
       "      <td>-0.073282</td>\n",
       "      <td>-0.366219</td>\n",
       "      <td>0.276971</td>\n",
       "      <td>0.185768</td>\n",
       "      <td>0.370330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>0.732881</td>\n",
       "      <td>-0.411135</td>\n",
       "      <td>-1.146540</td>\n",
       "      <td>-0.228638</td>\n",
       "      <td>-0.708022</td>\n",
       "      <td>0.510345</td>\n",
       "      <td>-0.543034</td>\n",
       "      <td>-1.048977</td>\n",
       "      <td>0.204206</td>\n",
       "      <td>0.572243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768727</td>\n",
       "      <td>-0.809597</td>\n",
       "      <td>0.663546</td>\n",
       "      <td>0.658442</td>\n",
       "      <td>0.572914</td>\n",
       "      <td>-0.675671</td>\n",
       "      <td>-0.748439</td>\n",
       "      <td>0.425618</td>\n",
       "      <td>0.471197</td>\n",
       "      <td>0.570765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>0.864386</td>\n",
       "      <td>-0.393110</td>\n",
       "      <td>-1.160002</td>\n",
       "      <td>-0.236677</td>\n",
       "      <td>-0.608832</td>\n",
       "      <td>0.306630</td>\n",
       "      <td>-0.474970</td>\n",
       "      <td>-0.335548</td>\n",
       "      <td>0.083655</td>\n",
       "      <td>0.481198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271284</td>\n",
       "      <td>-0.701424</td>\n",
       "      <td>0.943604</td>\n",
       "      <td>0.688306</td>\n",
       "      <td>0.535494</td>\n",
       "      <td>-0.303078</td>\n",
       "      <td>-0.631851</td>\n",
       "      <td>0.642840</td>\n",
       "      <td>0.394988</td>\n",
       "      <td>0.505712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "shall  0.542385 -0.297696 -0.974339 -0.637744 -0.677178  0.773788 -0.769360   \n",
       "unto   0.569215 -0.628994 -0.764580 -0.506907 -0.826138  0.275083 -0.601900   \n",
       "lord   0.353029 -0.589287 -0.441116 -0.433910 -0.473335  0.357725 -0.486547   \n",
       "thou   0.732881 -0.411135 -1.146540 -0.228638 -0.708022  0.510345 -0.543034   \n",
       "thy    0.864386 -0.393110 -1.160002 -0.236677 -0.608832  0.306630 -0.474970   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "shall -0.685529  0.758910  0.539016  ... -0.461902 -0.355086  0.454457   \n",
       "unto  -0.503314  0.685744  0.364418  ... -0.689491 -0.305249  0.546823   \n",
       "lord  -0.431751  0.626495  0.349939  ... -0.384809 -0.363280  0.142838   \n",
       "thou  -1.048977  0.204206  0.572243  ... -0.768727 -0.809597  0.663546   \n",
       "thy   -0.335548  0.083655  0.481198  ... -0.271284 -0.701424  0.943604   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "shall  0.599683  0.033022 -0.347140 -0.250258  0.407482  0.553953  0.660755  \n",
       "unto   0.690898  0.119913 -0.166880 -0.507530  0.522185  0.071481  0.323034  \n",
       "lord   0.070567  0.219064 -0.073282 -0.366219  0.276971  0.185768  0.370330  \n",
       "thou   0.658442  0.572914 -0.675671 -0.748439  0.425618  0.471197  0.570765  \n",
       "thy    0.688306  0.535494 -0.303078 -0.631851  0.642840  0.394988  0.505712  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights, index=list(id2word.values())).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = euclidean_distances(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12424, 12424)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement word2vec using Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(norm_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king james bible',\n",
       " 'old testament king james bible',\n",
       " 'first book moses called genesis',\n",
       " 'beginning god created heaven earth',\n",
       " 'earth without form void darkness upon face deep']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id) + 1\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skip-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build  the skip-grams model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embed_size, \n",
    "                         embeddings_initializer='glorot_uniform', \n",
    "                         input_length=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.add(Reshape((embed_size, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embed_size, \n",
    "                            embeddings_initializer='glorot_uniform',\n",
    "                            input_length=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model.add(Reshape((embed_size, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Concatenate([word_model, context_model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \"\"\"\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   1578\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams[:2]):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vectors = len(nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([word for sublist in [doc.split() for doc in norm_corpus] for word in sublist]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>-1.597225</td>\n",
       "      <td>-0.764704</td>\n",
       "      <td>-1.936276</td>\n",
       "      <td>-3.709085</td>\n",
       "      <td>0.872239</td>\n",
       "      <td>-0.122071</td>\n",
       "      <td>3.515677</td>\n",
       "      <td>0.086003</td>\n",
       "      <td>-1.459590</td>\n",
       "      <td>-3.823339</td>\n",
       "      <td>...</td>\n",
       "      <td>4.499367</td>\n",
       "      <td>1.891068</td>\n",
       "      <td>2.969626</td>\n",
       "      <td>0.074327</td>\n",
       "      <td>8.217146</td>\n",
       "      <td>2.430661</td>\n",
       "      <td>-0.801116</td>\n",
       "      <td>-0.229828</td>\n",
       "      <td>1.598217</td>\n",
       "      <td>-1.499999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>4.982570</td>\n",
       "      <td>2.689378</td>\n",
       "      <td>1.083143</td>\n",
       "      <td>-3.210576</td>\n",
       "      <td>1.034404</td>\n",
       "      <td>-0.194687</td>\n",
       "      <td>0.956865</td>\n",
       "      <td>2.572973</td>\n",
       "      <td>1.367883</td>\n",
       "      <td>0.291178</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.141736</td>\n",
       "      <td>1.183726</td>\n",
       "      <td>2.951894</td>\n",
       "      <td>-2.994605</td>\n",
       "      <td>-0.880541</td>\n",
       "      <td>3.039500</td>\n",
       "      <td>-0.512582</td>\n",
       "      <td>0.372515</td>\n",
       "      <td>-1.360234</td>\n",
       "      <td>-1.183977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bacon</th>\n",
       "      <td>4.888225</td>\n",
       "      <td>-1.579860</td>\n",
       "      <td>2.250744</td>\n",
       "      <td>-3.445150</td>\n",
       "      <td>-1.313023</td>\n",
       "      <td>-2.745318</td>\n",
       "      <td>0.422233</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>-2.258757</td>\n",
       "      <td>3.166235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451559</td>\n",
       "      <td>0.572528</td>\n",
       "      <td>6.394581</td>\n",
       "      <td>0.679203</td>\n",
       "      <td>3.810832</td>\n",
       "      <td>1.650661</td>\n",
       "      <td>-1.656798</td>\n",
       "      <td>-0.078714</td>\n",
       "      <td>-2.438453</td>\n",
       "      <td>3.246973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beans</th>\n",
       "      <td>1.690856</td>\n",
       "      <td>0.734803</td>\n",
       "      <td>1.700305</td>\n",
       "      <td>-2.660996</td>\n",
       "      <td>-1.612718</td>\n",
       "      <td>0.183725</td>\n",
       "      <td>0.976052</td>\n",
       "      <td>1.818056</td>\n",
       "      <td>-0.994682</td>\n",
       "      <td>0.379320</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.190541</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>3.917042</td>\n",
       "      <td>-0.986076</td>\n",
       "      <td>0.645866</td>\n",
       "      <td>1.322174</td>\n",
       "      <td>-0.043363</td>\n",
       "      <td>0.986605</td>\n",
       "      <td>-0.252803</td>\n",
       "      <td>-0.946032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>5.752971</td>\n",
       "      <td>-1.369993</td>\n",
       "      <td>2.740354</td>\n",
       "      <td>-2.547041</td>\n",
       "      <td>-2.789080</td>\n",
       "      <td>-0.963039</td>\n",
       "      <td>0.565059</td>\n",
       "      <td>-1.000548</td>\n",
       "      <td>-2.761723</td>\n",
       "      <td>-0.424854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456828</td>\n",
       "      <td>-2.432290</td>\n",
       "      <td>5.886449</td>\n",
       "      <td>-1.462701</td>\n",
       "      <td>1.469908</td>\n",
       "      <td>-0.907471</td>\n",
       "      <td>-1.432700</td>\n",
       "      <td>-1.795017</td>\n",
       "      <td>-2.538610</td>\n",
       "      <td>3.624875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>-1.199790</td>\n",
       "      <td>-0.908893</td>\n",
       "      <td>-0.677525</td>\n",
       "      <td>-1.492635</td>\n",
       "      <td>-1.008928</td>\n",
       "      <td>3.793826</td>\n",
       "      <td>2.799009</td>\n",
       "      <td>2.702869</td>\n",
       "      <td>-1.443188</td>\n",
       "      <td>-1.048369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535821</td>\n",
       "      <td>2.496920</td>\n",
       "      <td>2.910364</td>\n",
       "      <td>-1.421560</td>\n",
       "      <td>5.984182</td>\n",
       "      <td>0.564863</td>\n",
       "      <td>-1.832180</td>\n",
       "      <td>-3.960437</td>\n",
       "      <td>1.194844</td>\n",
       "      <td>1.772054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.773026</td>\n",
       "      <td>-2.011892</td>\n",
       "      <td>-3.083409</td>\n",
       "      <td>-3.498829</td>\n",
       "      <td>2.212975</td>\n",
       "      <td>0.228651</td>\n",
       "      <td>3.557877</td>\n",
       "      <td>0.743636</td>\n",
       "      <td>-0.642963</td>\n",
       "      <td>-0.272392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.673550</td>\n",
       "      <td>0.466299</td>\n",
       "      <td>4.076053</td>\n",
       "      <td>-0.731112</td>\n",
       "      <td>3.312188</td>\n",
       "      <td>2.540089</td>\n",
       "      <td>-1.560784</td>\n",
       "      <td>-1.713368</td>\n",
       "      <td>-0.120709</td>\n",
       "      <td>3.500930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lazy</th>\n",
       "      <td>-1.096738</td>\n",
       "      <td>0.809550</td>\n",
       "      <td>-0.847940</td>\n",
       "      <td>-2.706777</td>\n",
       "      <td>1.270977</td>\n",
       "      <td>-0.360892</td>\n",
       "      <td>3.787191</td>\n",
       "      <td>0.205429</td>\n",
       "      <td>-0.771524</td>\n",
       "      <td>-1.649072</td>\n",
       "      <td>...</td>\n",
       "      <td>3.817390</td>\n",
       "      <td>-0.548150</td>\n",
       "      <td>5.858268</td>\n",
       "      <td>-2.256131</td>\n",
       "      <td>3.164146</td>\n",
       "      <td>2.249343</td>\n",
       "      <td>-4.017901</td>\n",
       "      <td>-3.003519</td>\n",
       "      <td>1.266806</td>\n",
       "      <td>1.364671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breakfast</th>\n",
       "      <td>6.556323</td>\n",
       "      <td>-1.651479</td>\n",
       "      <td>1.498230</td>\n",
       "      <td>-4.397271</td>\n",
       "      <td>-0.939665</td>\n",
       "      <td>-1.912575</td>\n",
       "      <td>-0.321351</td>\n",
       "      <td>-1.432453</td>\n",
       "      <td>-3.410478</td>\n",
       "      <td>3.026450</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162866</td>\n",
       "      <td>-1.168921</td>\n",
       "      <td>5.796450</td>\n",
       "      <td>-2.759970</td>\n",
       "      <td>1.715276</td>\n",
       "      <td>0.597054</td>\n",
       "      <td>1.415331</td>\n",
       "      <td>-1.861535</td>\n",
       "      <td>-1.444216</td>\n",
       "      <td>7.522687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick</th>\n",
       "      <td>-0.598665</td>\n",
       "      <td>-1.158306</td>\n",
       "      <td>-3.909992</td>\n",
       "      <td>-0.372395</td>\n",
       "      <td>1.947247</td>\n",
       "      <td>-0.062387</td>\n",
       "      <td>2.115516</td>\n",
       "      <td>-0.791645</td>\n",
       "      <td>-0.281300</td>\n",
       "      <td>-0.309056</td>\n",
       "      <td>...</td>\n",
       "      <td>3.075487</td>\n",
       "      <td>0.889505</td>\n",
       "      <td>3.620715</td>\n",
       "      <td>-1.092861</td>\n",
       "      <td>3.872988</td>\n",
       "      <td>2.059922</td>\n",
       "      <td>-1.751327</td>\n",
       "      <td>-1.977854</td>\n",
       "      <td>-0.352194</td>\n",
       "      <td>-0.584710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>7.615616</td>\n",
       "      <td>-0.896800</td>\n",
       "      <td>1.907612</td>\n",
       "      <td>-3.675507</td>\n",
       "      <td>-1.785053</td>\n",
       "      <td>-1.830063</td>\n",
       "      <td>-1.030577</td>\n",
       "      <td>-1.294997</td>\n",
       "      <td>-5.137047</td>\n",
       "      <td>0.314907</td>\n",
       "      <td>...</td>\n",
       "      <td>1.937132</td>\n",
       "      <td>0.588607</td>\n",
       "      <td>8.159275</td>\n",
       "      <td>-0.592951</td>\n",
       "      <td>0.060248</td>\n",
       "      <td>-0.278584</td>\n",
       "      <td>-1.384910</td>\n",
       "      <td>-1.170500</td>\n",
       "      <td>-1.557352</td>\n",
       "      <td>4.594214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toast</th>\n",
       "      <td>1.843521</td>\n",
       "      <td>-0.269774</td>\n",
       "      <td>0.256220</td>\n",
       "      <td>-2.010448</td>\n",
       "      <td>-0.383293</td>\n",
       "      <td>-1.248663</td>\n",
       "      <td>-0.452082</td>\n",
       "      <td>-4.066278</td>\n",
       "      <td>-4.177999</td>\n",
       "      <td>0.800663</td>\n",
       "      <td>...</td>\n",
       "      <td>2.121760</td>\n",
       "      <td>0.821847</td>\n",
       "      <td>6.710506</td>\n",
       "      <td>-0.511356</td>\n",
       "      <td>2.052958</td>\n",
       "      <td>1.860764</td>\n",
       "      <td>-1.228027</td>\n",
       "      <td>-1.802264</td>\n",
       "      <td>-1.454504</td>\n",
       "      <td>4.086023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>-0.465194</td>\n",
       "      <td>-1.094278</td>\n",
       "      <td>-3.627848</td>\n",
       "      <td>-0.217452</td>\n",
       "      <td>-0.146711</td>\n",
       "      <td>2.360640</td>\n",
       "      <td>0.994361</td>\n",
       "      <td>2.341765</td>\n",
       "      <td>-0.175683</td>\n",
       "      <td>0.600772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432553</td>\n",
       "      <td>1.125682</td>\n",
       "      <td>3.314736</td>\n",
       "      <td>-3.134841</td>\n",
       "      <td>5.482524</td>\n",
       "      <td>3.531374</td>\n",
       "      <td>1.676525</td>\n",
       "      <td>-0.800145</td>\n",
       "      <td>1.396107</td>\n",
       "      <td>1.461131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sausages</th>\n",
       "      <td>8.376391</td>\n",
       "      <td>-0.630400</td>\n",
       "      <td>5.057930</td>\n",
       "      <td>-3.044060</td>\n",
       "      <td>0.521349</td>\n",
       "      <td>-0.206568</td>\n",
       "      <td>-0.593800</td>\n",
       "      <td>-3.094994</td>\n",
       "      <td>-1.111348</td>\n",
       "      <td>4.193744</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.713156</td>\n",
       "      <td>-0.351658</td>\n",
       "      <td>3.665479</td>\n",
       "      <td>-2.118978</td>\n",
       "      <td>-2.468455</td>\n",
       "      <td>0.817962</td>\n",
       "      <td>-1.506413</td>\n",
       "      <td>1.194050</td>\n",
       "      <td>-3.645459</td>\n",
       "      <td>-0.109955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kings</th>\n",
       "      <td>1.963727</td>\n",
       "      <td>1.400355</td>\n",
       "      <td>-0.212680</td>\n",
       "      <td>-3.240910</td>\n",
       "      <td>-0.011279</td>\n",
       "      <td>1.192754</td>\n",
       "      <td>0.069450</td>\n",
       "      <td>1.099375</td>\n",
       "      <td>0.041750</td>\n",
       "      <td>1.710340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.383223</td>\n",
       "      <td>0.580553</td>\n",
       "      <td>2.143755</td>\n",
       "      <td>-1.957663</td>\n",
       "      <td>-1.925572</td>\n",
       "      <td>0.231861</td>\n",
       "      <td>0.611520</td>\n",
       "      <td>2.591057</td>\n",
       "      <td>-1.253806</td>\n",
       "      <td>-0.510482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>4.400901</td>\n",
       "      <td>-1.673245</td>\n",
       "      <td>-0.304506</td>\n",
       "      <td>-0.771645</td>\n",
       "      <td>-4.335264</td>\n",
       "      <td>-1.201353</td>\n",
       "      <td>-0.319782</td>\n",
       "      <td>-1.141011</td>\n",
       "      <td>-3.969310</td>\n",
       "      <td>-0.277807</td>\n",
       "      <td>...</td>\n",
       "      <td>1.913574</td>\n",
       "      <td>-1.262062</td>\n",
       "      <td>3.334505</td>\n",
       "      <td>-3.181927</td>\n",
       "      <td>0.597050</td>\n",
       "      <td>0.218071</td>\n",
       "      <td>1.808904</td>\n",
       "      <td>-3.048329</td>\n",
       "      <td>-1.868089</td>\n",
       "      <td>3.788952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>1.951794</td>\n",
       "      <td>-0.997128</td>\n",
       "      <td>-0.433150</td>\n",
       "      <td>-2.571720</td>\n",
       "      <td>-1.737310</td>\n",
       "      <td>-0.472975</td>\n",
       "      <td>0.304337</td>\n",
       "      <td>-1.050770</td>\n",
       "      <td>-0.790423</td>\n",
       "      <td>0.539001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135221</td>\n",
       "      <td>-0.015413</td>\n",
       "      <td>5.036502</td>\n",
       "      <td>-2.552612</td>\n",
       "      <td>1.525144</td>\n",
       "      <td>3.253965</td>\n",
       "      <td>0.314473</td>\n",
       "      <td>-2.270473</td>\n",
       "      <td>-1.755431</td>\n",
       "      <td>2.271016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>2.668190</td>\n",
       "      <td>1.205964</td>\n",
       "      <td>-2.616678</td>\n",
       "      <td>0.162358</td>\n",
       "      <td>-3.444298</td>\n",
       "      <td>-0.131091</td>\n",
       "      <td>-1.533019</td>\n",
       "      <td>-2.176539</td>\n",
       "      <td>-2.421778</td>\n",
       "      <td>-0.155488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271191</td>\n",
       "      <td>-2.902583</td>\n",
       "      <td>6.282207</td>\n",
       "      <td>-0.947092</td>\n",
       "      <td>1.038567</td>\n",
       "      <td>0.220829</td>\n",
       "      <td>-1.220949</td>\n",
       "      <td>-1.417845</td>\n",
       "      <td>-0.590916</td>\n",
       "      <td>5.657127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>4.958938</td>\n",
       "      <td>0.159355</td>\n",
       "      <td>-0.783993</td>\n",
       "      <td>-1.755608</td>\n",
       "      <td>3.229049</td>\n",
       "      <td>-0.333242</td>\n",
       "      <td>-0.118168</td>\n",
       "      <td>0.961278</td>\n",
       "      <td>-3.132787</td>\n",
       "      <td>4.051391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555337</td>\n",
       "      <td>0.596979</td>\n",
       "      <td>4.215394</td>\n",
       "      <td>-3.182428</td>\n",
       "      <td>-1.473035</td>\n",
       "      <td>1.674594</td>\n",
       "      <td>1.737822</td>\n",
       "      <td>0.027312</td>\n",
       "      <td>-2.989328</td>\n",
       "      <td>-0.569634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>5.824707</td>\n",
       "      <td>0.162262</td>\n",
       "      <td>0.657427</td>\n",
       "      <td>-4.248973</td>\n",
       "      <td>-1.101111</td>\n",
       "      <td>-2.680099</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>-1.564659</td>\n",
       "      <td>-1.537709</td>\n",
       "      <td>0.962514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131793</td>\n",
       "      <td>0.538094</td>\n",
       "      <td>5.617314</td>\n",
       "      <td>-2.078991</td>\n",
       "      <td>0.543525</td>\n",
       "      <td>1.268044</td>\n",
       "      <td>-1.466586</td>\n",
       "      <td>-1.380141</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>8.017013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "beautiful -1.597225 -0.764704 -1.936276 -3.709085  0.872239 -0.122071   \n",
       "jumps      4.982570  2.689378  1.083143 -3.210576  1.034404 -0.194687   \n",
       "bacon      4.888225 -1.579860  2.250744 -3.445150 -1.313023 -2.745318   \n",
       "beans      1.690856  0.734803  1.700305 -2.660996 -1.612718  0.183725   \n",
       "love       5.752971 -1.369993  2.740354 -2.547041 -2.789080 -0.963039   \n",
       "blue      -1.199790 -0.908893 -0.677525 -1.492635 -1.008928  3.793826   \n",
       "brown      0.773026 -2.011892 -3.083409 -3.498829  2.212975  0.228651   \n",
       "lazy      -1.096738  0.809550 -0.847940 -2.706777  1.270977 -0.360892   \n",
       "breakfast  6.556323 -1.651479  1.498230 -4.397271 -0.939665 -1.912575   \n",
       "quick     -0.598665 -1.158306 -3.909992 -0.372395  1.947247 -0.062387   \n",
       "sky        7.615616 -0.896800  1.907612 -3.675507 -1.785053 -1.830063   \n",
       "toast      1.843521 -0.269774  0.256220 -2.010448 -0.383293 -1.248663   \n",
       "green     -0.465194 -1.094278 -3.627848 -0.217452 -0.146711  2.360640   \n",
       "sausages   8.376391 -0.630400  5.057930 -3.044060  0.521349 -0.206568   \n",
       "kings      1.963727  1.400355 -0.212680 -3.240910 -0.011279  1.192754   \n",
       "today      4.400901 -1.673245 -0.304506 -0.771645 -4.335264 -1.201353   \n",
       "fox        1.951794 -0.997128 -0.433150 -2.571720 -1.737310 -0.472975   \n",
       "ham        2.668190  1.205964 -2.616678  0.162358 -3.444298 -0.131091   \n",
       "eggs       4.958938  0.159355 -0.783993 -1.755608  3.229049 -0.333242   \n",
       "dog        5.824707  0.162262  0.657427 -4.248973 -1.101111 -2.680099   \n",
       "\n",
       "                 6         7         8         9   ...        86        87  \\\n",
       "beautiful  3.515677  0.086003 -1.459590 -3.823339  ...  4.499367  1.891068   \n",
       "jumps      0.956865  2.572973  1.367883  0.291178  ... -1.141736  1.183726   \n",
       "bacon      0.422233  0.065403 -2.258757  3.166235  ...  0.451559  0.572528   \n",
       "beans      0.976052  1.818056 -0.994682  0.379320  ... -1.190541  0.086937   \n",
       "love       0.565059 -1.000548 -2.761723 -0.424854  ...  0.456828 -2.432290   \n",
       "blue       2.799009  2.702869 -1.443188 -1.048369  ...  0.535821  2.496920   \n",
       "brown      3.557877  0.743636 -0.642963 -0.272392  ...  3.673550  0.466299   \n",
       "lazy       3.787191  0.205429 -0.771524 -1.649072  ...  3.817390 -0.548150   \n",
       "breakfast -0.321351 -1.432453 -3.410478  3.026450  ...  2.162866 -1.168921   \n",
       "quick      2.115516 -0.791645 -0.281300 -0.309056  ...  3.075487  0.889505   \n",
       "sky       -1.030577 -1.294997 -5.137047  0.314907  ...  1.937132  0.588607   \n",
       "toast     -0.452082 -4.066278 -4.177999  0.800663  ...  2.121760  0.821847   \n",
       "green      0.994361  2.341765 -0.175683  0.600772  ...  0.432553  1.125682   \n",
       "sausages  -0.593800 -3.094994 -1.111348  4.193744  ... -2.713156 -0.351658   \n",
       "kings      0.069450  1.099375  0.041750  1.710340  ... -1.383223  0.580553   \n",
       "today     -0.319782 -1.141011 -3.969310 -0.277807  ...  1.913574 -1.262062   \n",
       "fox        0.304337 -1.050770 -0.790423  0.539001  ...  0.135221 -0.015413   \n",
       "ham       -1.533019 -2.176539 -2.421778 -0.155488  ...  0.271191 -2.902583   \n",
       "eggs      -0.118168  0.961278 -3.132787  4.051391  ... -0.555337  0.596979   \n",
       "dog        0.004892 -1.564659 -1.537709  0.962514  ...  1.131793  0.538094   \n",
       "\n",
       "                 88        89        90        91        92        93  \\\n",
       "beautiful  2.969626  0.074327  8.217146  2.430661 -0.801116 -0.229828   \n",
       "jumps      2.951894 -2.994605 -0.880541  3.039500 -0.512582  0.372515   \n",
       "bacon      6.394581  0.679203  3.810832  1.650661 -1.656798 -0.078714   \n",
       "beans      3.917042 -0.986076  0.645866  1.322174 -0.043363  0.986605   \n",
       "love       5.886449 -1.462701  1.469908 -0.907471 -1.432700 -1.795017   \n",
       "blue       2.910364 -1.421560  5.984182  0.564863 -1.832180 -3.960437   \n",
       "brown      4.076053 -0.731112  3.312188  2.540089 -1.560784 -1.713368   \n",
       "lazy       5.858268 -2.256131  3.164146  2.249343 -4.017901 -3.003519   \n",
       "breakfast  5.796450 -2.759970  1.715276  0.597054  1.415331 -1.861535   \n",
       "quick      3.620715 -1.092861  3.872988  2.059922 -1.751327 -1.977854   \n",
       "sky        8.159275 -0.592951  0.060248 -0.278584 -1.384910 -1.170500   \n",
       "toast      6.710506 -0.511356  2.052958  1.860764 -1.228027 -1.802264   \n",
       "green      3.314736 -3.134841  5.482524  3.531374  1.676525 -0.800145   \n",
       "sausages   3.665479 -2.118978 -2.468455  0.817962 -1.506413  1.194050   \n",
       "kings      2.143755 -1.957663 -1.925572  0.231861  0.611520  2.591057   \n",
       "today      3.334505 -3.181927  0.597050  0.218071  1.808904 -3.048329   \n",
       "fox        5.036502 -2.552612  1.525144  3.253965  0.314473 -2.270473   \n",
       "ham        6.282207 -0.947092  1.038567  0.220829 -1.220949 -1.417845   \n",
       "eggs       4.215394 -3.182428 -1.473035  1.674594  1.737822  0.027312   \n",
       "dog        5.617314 -2.078991  0.543525  1.268044 -1.466586 -1.380141   \n",
       "\n",
       "                 94        95  \n",
       "beautiful  1.598217 -1.499999  \n",
       "jumps     -1.360234 -1.183977  \n",
       "bacon     -2.438453  3.246973  \n",
       "beans     -0.252803 -0.946032  \n",
       "love      -2.538610  3.624875  \n",
       "blue       1.194844  1.772054  \n",
       "brown     -0.120709  3.500930  \n",
       "lazy       1.266806  1.364671  \n",
       "breakfast -1.444216  7.522687  \n",
       "quick     -0.352194 -0.584710  \n",
       "sky       -1.557352  4.594214  \n",
       "toast     -1.454504  4.086023  \n",
       "green      1.396107  1.461131  \n",
       "sausages  -3.645459 -0.109955  \n",
       "kings     -1.253806 -0.510482  \n",
       "today     -1.868089  3.788952  \n",
       "fox       -1.755431  2.271016  \n",
       "ham       -0.590916  5.657127  \n",
       "eggs      -2.989328 -0.569634  \n",
       "dog       -0.598523  8.017013  \n",
       "\n",
       "[20 rows x 96 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_glove_vectors, index=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
